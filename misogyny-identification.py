# -*- coding: utf-8 -*-
"""misogyny_identification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q_ecFhKc__R7K_aZJcu0pOeNgEk8OgQG
"""

! pip install -q pyspark==3.1.2 spark-nlp
! pip install bnlp_toolkit
! pip install -U bnlp_toolkit

! pip install emoji

!pip install bangla-stemmer

!pip3 install -q pyspark==3.1.2 spark-nlp

from bnlp import NLTKTokenizer
from bnlp.corpus import stopwords, punctuations, letters, digits
from bnlp.corpus import stopwords
from bnlp.corpus.util import remove_stopwords
from bangla_stemmer.stemmer import stemmer
from gensim.models import Word2Vec
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sklearn
import emoji
import re
import nltk
import bnlp
import heapq
import json
import pickle
from pyspark.ml import Pipeline
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from sparknlp.annotator import *
from sparknlp.base import *
import sparknlp
from sparknlp.pretrained import PretrainedPipeline
from bnlp.corpus import stopwords, punctuations, letters, digits
#bnlp.download('stopwords')
#bnlp.download('wordnet')
#bnlp.download('averaged_perceptron_tagger')
#bnlp.download('maxent_ne_chunker')
#bnlp.download('words')
#bnlp.download('punkt')

spark = sparknlp.start()

pwd

#importing Dataset
dataset = pd.read_csv(r'/content/sample_data/Misogyny Classification - Sheet1.csv')
dataset.head()

for col in dataset.columns:
    print(col, dataset[col].isnull().sum())

dataset.shape

dataset.dropna()
dataset.head()

dataset.to_csv(r'/content/sample_data/Misogyny Classification - Sheet1.csv', encoding='utf-8')

dataset = pd.read_csv(r'/content/sample_data/Misogyny Classification - Sheet1.csv')
dataset.drop('Unnamed: 0', axis=1, inplace=True)
dataset = dataset.loc[:, ~dataset.columns.str.contains('^Unnamed')]

dataset.head()

text = dataset['Text'].tolist()
m_lvl =dataset['Misogynistic'].tolist()
#s_lvl = dataset['Non-Misogynistic'].tolist()
print(text)
print(m_lvl)
#print(s_lvl)

print(len(text))

#creating the corpus
#corpus = []
#corpus2 = []
#corpus3 = []
#corpus4 = []
#sentences = []
#for i in range(0,len(X)):
#  review = re.sub(r'\W',' ',str(X[i]))
#  review = review.lower()
#  review = re.sub(r'\s+[a-z]\s+',' ',review)
#  review = re.sub(r'^[a-z]\s+',' ',review)
#  review = re.sub(r'\s+',' ',review)
#  review = re.sub(r'\d',' ',review)
#  review = re.sub(r'\s+[a-z]\s+',' ',review)
#  review = re.sub(r'\s+',' ',review)
#  review = re.sub(r'^\s','',review)
#  review = re.sub(r'\s$','',review)
#  corpus.append(review)
#corpus2 = corpus
#corpus3 = corpus
#corpus4 = corpus
#sentences = corpus4
#print(corpus)
#print(len(corpus))

def remove_numbers(text):
    text = re.sub(r"\d","",text)

print(text)

# remove numbers
def remove_numbers(text):
    text = "".join(i for i in text if i in ["।"]
            or 2432 <= ord(i) <= 2531 or ord(i) == 32)
    text = " ".join(text.split())
    return text

print(text)

import string
from bnlp.corpus import stopwords
from bnlp.corpus.util import remove_stopwords
from bnlp.corpus import stopwords, punctuations, letters, digits

print(stopwords)
print(punctuations)
print(letters)
print(digits)

raw_text = 'আমি ভাত খাই। ১২ ১৪ করিস না মাগি ১ +.... / ? .....'
new_text = raw_text.translate(str.maketrans('', '', string.punctuation))
result = remove_stopwords(raw_text, stopwords)

remove_digits = str.maketrans('', '', digits)
res = raw_text.translate(remove_digits)

print(res)

#def remove_punctuations(raw_text):
#  raw_text_nopunc = "".join([c for c in raw_text if c not in punctuations])
#  return raw_text_nopunc


#dataset['clean_text'] = dataset['Text'].apply(lambda x: remove_punctuations(x))
#dataset.head()


print(result)
print(new_text)
# ['ভাত', 'খাই', '।']

dataset.head()





import string
from bnlp.corpus import stopwords
from bnlp.corpus.util import remove_stopwords
from bnlp.corpus import stopwords, punctuations, letters, digits

print(stopwords)
print(punctuations)
print(letters)
print(digits)

raw_text = 'আমি ভাত খাই। ১২ ১৪ করিস না মাগি ১ +.... / ? .....'
new_text = raw_text.translate(str.maketrans('', '', string.punctuation))
result = remove_stopwords(raw_text, stopwords)

remove_digits = str.maketrans('', '', digits)
res = raw_text.translate(remove_digits)

print(res)

#def remove_punctuations(raw_text):
#  raw_text_nopunc = "".join([c for c in raw_text if c not in punctuations])
#  return raw_text_nopunc


#dataset['clean_text'] = dataset['Text'].apply(lambda x: remove_punctuations(x))
#dataset.head()

def remove_digits(text):
  remove_digits = str.maketrans('','',digits)
  text = text.translate(remove_digits)
  return text




print(result)
print(new_text)
print(text)
# ['ভাত', 'খাই', '।']

# remove numbers
def remove_numbers(text):
    text = "".join(i for i in text if i in ["।"]
            or 2432 <= ord(i) <= 2531 or ord(i) == 32)
    text = " ".join(text.split())
    return text

# remove punctuation
def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)

def bangla_stemmer(text): 
    temp =[]
    for x in text: 
        stmr = stemmer.BanglaStemmer()
        stm = stmr.stem(x)
        temp.append(stm)
    return temp

document_assembler = DocumentAssembler() \
    .setInputCol("text") \
    .setOutputCol("document")

tokenizer = Tokenizer()\
    .setInputCols(["document"]) \
    .setOutputCol("token")

lemmatizer = LemmatizerModel.pretrained("lemma", "bn") \
    .setInputCols(["token"]) \
    .setOutputCol("lemma")


nlp_pipeline = Pipeline(stages=[document_assembler, tokenizer, lemmatizer])

light_pipeline = LightPipeline(nlp_pipeline.fit(
    spark.createDataFrame([[""]]).toDF("text")))

data_lst = []
def lemmatize_bangla(text):
  results = light_pipeline.fullAnnotate(text)
  for k, v in results[0].items():
    for item in range(len(v)):
      data_lst.append(v[item].result)
  return data_lst

# lemmatize
# lemmatizer = WordNetLemmatizer()

def lemmatize(text):
    text = [lemmatizer.lemmatize(token) for token in text]
    return text

print(text)

def remove_emoji(text):
    return emoji.get_emoji_regexp().sub(u'', text)

def join_text(text_list):
    temp =  ' '.join(text_list);
    return temp

#emoji remove
temp = []
#for x in text:  
#    temp.append(remove_emoji(x))

#text = temp.copy()
#temp.clear()

#punctuation remove
#print(temp)
for x in text:
    temp.append(remove_stopwords(x, punctuations))

text = temp.copy()
temp.clear()

for x in text:
    temp.append(join_text(x))

text = temp.copy()
temp.clear()

# remove numbers
for x in text:
    temp.append(remove_numbers(x))


text = temp.copy()
temp.clear()

print(text)

#remove stopwords4
def tokenize(self, text):
  temp = []
  for x in text:
      temp.append(remove_stopwords(x, stopwords))

  text = temp.copy()
  temp.clear()

#stemmer 
def bangla_stemmer(text):
  temp =[]
  for x in text:
    stmr = stemmer.BanglaStemmer()
    stm = stmr.stem(x)
  for x in text:
    temp.append(bangla_stemmer(x))

  text = temp.copy()
  temp.clear()

# rejoin the words as sentance
def join_text(text_list):
  for x in text:
      temp.append(join_text(x))

  text = temp.copy()
  temp.clear()
#print(text)

# remove stopwords
for x in text:
    temp.append(remove_stopwords(x,stopwords))


text = temp.copy()
temp.clear()

print(text)

text2 = ([' '.join(i) for i in text])
print(text2)

print(len(text2))

flat_list = []
for sublist in text:
    for item in sublist:
        flat_list.append(item)

print(flat_list)

#creating histogram
from bnlp import NLTKTokenizer
bnltk = NLTKTokenizer()
word2count={}
senctence2count={}
for data in text2:
  words = bnltk.word_tokenize(data)
  for word in words:
    if word not in word2count.keys():
      word2count[word]=1
    else:
      word2count[word]+=1

for data in text2:
  sentence = bnltk.sentence_tokenize(data)
  for word in sentence:
    if word not in senctence2count.keys():
      senctence2count[word]=1
    else:
      senctence2count[word]+=1

print(word2count)  
print(len(word2count))   

print(senctence2count)  
print(len(senctence2count))

#finding out most frequent words
freq_words = heapq.nlargest(350,word2count,key=word2count.get)
print(freq_words)
print(len(freq_words))

X = dataset['Text'].tolist();
Y = dataset['Misogynistic'].tolist();
print(X)
print(Y)

#storing X and Y as pickle files
with open('text2.pickle','wb') as f:
  pickle.dump(X,f)

with open('Y.pickle','wb') as f:
  pickle.dump(Y,f)

#unpickling the dataset
with open('text2.pickle','rb') as f:
  X=pickle.load(f)

with open('Y.pickle','rb') as f:
  Y=pickle.load(f)

#building bag of words model
BOWM = []
for data in text2:
  vector = []
  for word in freq_words:
    if word in bnltk.word_tokenize(data):
      vector.append(1)
    else:
      vector.append(0)
    BOWM.append(vector)
BOWM = np.asarray(BOWM)
print(BOWM)
print(len(BOWM))

#TF Matrix

tf_matrix={}
for word in freq_words:
  doc_tf = []
  for data in text2:
    frequency = 0
    for w in bnltk.word_tokenize(data):
      if w == word:
        frequency += 1
    if len(bnltk.word_tokenize(data))!= 0:
      tf_word = frequency/len(bnltk.word_tokenize(data))
      doc_tf.append(tf_word)
    tf_matrix[word] = doc_tf

#print(tf_matrix)
#print(len(tf_matrix))

len(text2)

len(Y)

#IDF Matrix

word_idfs = {}
for word in freq_words:
  doc_count=0
  for data in text2:
    if word in bnltk.word_tokenize(data):
      doc_count+=1
  word_idfs[word] = np.log((len(text)/doc_count)+1)

print(word_idfs)
print(len(word_idfs))

#TF-IDF calculation

tfidf_matrix = []
for word in tf_matrix.keys():
  tfidf = []
  for value in tf_matrix[word]:
    score = value*word_idfs[word]
    tfidf.append(score)
  tfidf_matrix.append(tfidf)

TFM = np.asarray(tfidf_matrix)
TFM = np.transpose(TFM)
print(TFM)

print(len(TFM))

print(X[0])
print(Y[0])
print(X[2089])
print(Y[1606])

dataset.shape

dataset.info()

dataset.dropna()

dataset.info()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    TFM, Y, test_size=0.1, random_state=42)

print(X_train)

print(y_train)

print(X_test)

print(y_test)

dataset.shape

dataset.info()

#LOGISTIC REGRESSION

from sklearn.linear_model import LogisticRegression
classifier_lr = LogisticRegression(random_state=42).fit(X_train, y_train)
y_prediction_lr = classifier_lr.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy_lr = accuracy_score(y_test, y_prediction_lr)

print(accuracy_lr)

from sklearn.metrics import precision_score
precision_lr = precision_score(y_test, y_prediction_lr, average='macro')
print(precision_lr)

from sklearn.metrics import recall_score
recall_lr = recall_score(y_test, y_prediction_lr, average='macro')
print(recall_lr)

from sklearn.metrics import f1_score
f1_score_lr = f1_score(y_test, y_prediction_lr, average='macro')
f1_score_lr2 = (2*precision_lr*recall_lr)/(precision_lr+recall_lr)
print(f1_score_lr)
print(f1_score_lr2)

#K-NEAREST NEIGHBORS (K-NN)

from sklearn.neighbors import KNeighborsClassifier
classifier_knn = KNeighborsClassifier(n_neighbors = 5,metric = 'minkowski',p=2)
classifier_knn.fit(X_train,y_train)
y_prediction_knn = classifier_knn.predict(X_test)

#from sklearn.metrics import accuracy_score

accuracy_knn = accuracy_score(y_test, y_prediction_knn)

print(accuracy_knn)

#from sklearn.metrics import precision_score
precision_knn = precision_score(y_test, y_prediction_knn, average='macro')
print(precision_knn)

#from sklearn.metrics import recall_score
recall_knn = recall_score(y_test, y_prediction_knn, average='macro')
print(recall_knn)

#from sklearn.metrics import f1_score
f1_score_knn = f1_score(y_test, y_prediction_knn, average='macro')
f1_score_knn2 = (2*precision_knn*recall_knn)/(precision_knn+recall_knn)
print(f1_score_knn)
print(f1_score_knn2)

#SUPPORT VECTOR MACHINE(SVM)

from sklearn.svm import SVC
classifier_svm = SVC(kernel = 'linear', random_state =0)
classifier_svm.fit(X_train,y_train)
y_prediction_svm = classifier_svm.predict(X_test)

#from sklearn.metrics import accuracy_score

accuracy_svm = accuracy_score(y_test, y_prediction_svm)

print(accuracy_svm)

#from sklearn.metrics import precision_score
precision_svm = precision_score(y_test, y_prediction_svm, average='macro')
print(precision_svm)

#from sklearn.metrics import recall_score
recall_svm = recall_score(y_test, y_prediction_svm, average='macro')
print(recall_svm)

#from sklearn.metrics import f1_score
f1_score_svm = f1_score(y_test, y_prediction_svm, average='macro')
f1_score_svm2 = (2*precision_svm*recall_svm)/(precision_svm+recall_svm)
print(f1_score_svm)
print(f1_score_svm2)

#NAIVE BAYES
from sklearn.naive_bayes import GaussianNB
classifier_NB= GaussianNB()
classifier_NB.fit(X_train,y_train)
y_prediction_NB = classifier_NB.predict(X_test)

accuracy_NB = accuracy_score(y_test, y_prediction_NB)

print(accuracy_NB)

#from sklearn.metrics import precision_score
precision_NB = precision_score(y_test, y_prediction_NB, average='macro')
print(precision_NB)

#from sklearn.metrics import recall_score
recall_NB = recall_score(y_test, y_prediction_NB, average='macro')
print(recall_NB)

#from sklearn.metrics import f1_score
f1_score_NB = f1_score(y_test, y_prediction_NB, average='macro')
f1_score_NB2 = (2*precision_NB*recall_NB)/(precision_NB+recall_NB)
print(f1_score_NB)
print(f1_score_NB2)

#DECISION TREE CLASSIFICATION

from sklearn.tree import DecisionTreeClassifier
classifier_dt = DecisionTreeClassifier(criterion = 'entropy',random_state=0)
classifier_dt.fit(X_train,y_train)
y_prediction_dt = classifier_dt.predict(X_test)

accuracy_dt = accuracy_score(y_test, y_prediction_dt)

print(accuracy_dt)

#from sklearn.metrics import precision_score
precision_dt = precision_score(y_test, y_prediction_dt, average='macro')
print(precision_dt)

#from sklearn.metrics import recall_score
recall_dt = recall_score(y_test, y_prediction_dt, average='macro')
print(recall_dt)

#from sklearn.metrics import f1_score
f1_score_dt = f1_score(y_test, y_prediction_dt, average='macro')
f1_score_dt2 = (2*precision_dt*recall_dt)/(precision_dt+recall_dt)
print(f1_score_dt)
print(f1_score_dt2)

#RANDOM FOREST CLASSIFICATION

from sklearn.ensemble import RandomForestClassifier
classifier_rf = RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state=0)
classifier_rf.fit(X_train,y_train)
y_prediction_rf = classifier_rf.predict(X_test)

accuracy_rf = accuracy_score(y_test, y_prediction_rf)

print(accuracy_rf)

#from sklearn.metrics import precision_score
precision_rf = precision_score(y_test, y_prediction_rf, average='macro')
print(precision_rf)

#from sklearn.metrics import recall_score
recall_rf = recall_score(y_test, y_prediction_rf, average='macro')
print(recall_rf)

#from sklearn.metrics import f1_score
f1_score_rf = f1_score(y_test, y_prediction_rf, average='macro')
f1_score_rf2 = (2*precision_rf*recall_rf)/(precision_rf+recall_rf)
print(f1_score_rf)
print(f1_score_rf2)